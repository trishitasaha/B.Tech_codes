{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "DqlF61YuvhFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUyhIemsvkRX",
        "outputId": "4ed352ed-e61f-441e-c4ed-7239de345633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique words"
      ],
      "metadata": {
        "id": "VzOM48eMvxWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import TRUE\n",
        "from nltk.corpus.reader import wordlist\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer as las\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "enc=[]\n",
        "ps=PorterStemmer()\n",
        "count = 0\n",
        "n=0\n",
        "en=0\n",
        "t=0\n",
        "file = open(\"Testing.txt\", \"r\")\n",
        "read_dataing = file.read()\n",
        "print(read_dataing)\n",
        "read_data = re.sub(r'[^\\w\\s]', '', read_dataing)\n",
        "text = word_tokenize(read_data)\n",
        "file=open(\"Testing.txt\")\n",
        "my_lines_list=file.readlines()\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(ps.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "#print(\"Rooted sentence\")\n",
        "x=stemSentence(my_lines_list[0])\n",
        "#print(x)\n",
        "M1 = word_tokenize(read_data)\n",
        "sen = nltk.pos_tag(M1)\n",
        "print(sen)\n",
        "words = Counter(read_data.split())\n",
        "for word in words:\n",
        "    count += 1\n",
        "for word in words:\n",
        "    n += 1\n",
        "    en=0\n",
        "    k=words[word]\n",
        "    token_comment = word_tokenize(word)\n",
        "    tagged_comment = pos_tag(token_comment)\n",
        "    for i in range(len(token_comment)):\n",
        "      for (word, tag) in tagged_comment:\n",
        "        if (tag=='CC'):\n",
        "          t=1\n",
        "        if (tag=='CD'):\n",
        "          t=2\n",
        "        if (tag=='DT'):\n",
        "          t=3\n",
        "        if (tag=='EX'):\n",
        "          t=4\n",
        "        if (tag=='FW'):\n",
        "          t=5\n",
        "        if (tag=='IN'):\n",
        "          t=6\n",
        "        if (tag=='JJ'):\n",
        "          t=7\n",
        "        if (tag=='JJR'):\n",
        "          t=8\n",
        "        if (tag=='JJS'):\n",
        "          t=9\n",
        "        if (tag=='LS'):\n",
        "          t=10\n",
        "        if (tag=='MD'):\n",
        "          t=11\n",
        "        if (tag=='NN'):\n",
        "          t=12\n",
        "        if (tag=='NNS'):\n",
        "          t=13\n",
        "        if (tag=='NNP'):\n",
        "          t=14\n",
        "        if (tag=='NNPS'):\n",
        "          t=15\n",
        "        if (tag=='PDT'):\n",
        "          t=16\n",
        "        if (tag=='POS'):\n",
        "          t=17\n",
        "        if (tag=='PRP'):\n",
        "          t=18\n",
        "        if (tag=='PRP$'):\n",
        "          t=19\n",
        "        if (tag=='RB'):\n",
        "          t=20\n",
        "        if (tag=='RBR'):\n",
        "          t=21\n",
        "        if (tag=='RBS'):\n",
        "          t=22\n",
        "        if (tag=='RP'):\n",
        "          t=23\n",
        "        if (tag=='TO'):\n",
        "          t=24\n",
        "        if (tag=='UH'):\n",
        "          t=25\n",
        "        if (tag=='VBP'):\n",
        "          t=26\n",
        "        if (tag=='VBD'):\n",
        "          t=27\n",
        "        if (tag=='VBG'):\n",
        "          t=28\n",
        "        if (tag=='VBN'):\n",
        "          t=29\n",
        "        if (tag=='VB'):\n",
        "          t=30\n",
        "        if (tag=='VBZ'):\n",
        "          t=31\n",
        "        if (tag=='WDT'):\n",
        "          t=32\n",
        "        if (tag=='WP'):\n",
        "          t=33\n",
        "        if (tag=='WP$'):\n",
        "          t=34\n",
        "        if (tag=='WRB'):\n",
        "          t=35\n",
        "    en=n+k+t\n",
        "    print(word,\"-\", \"n\", \"-\", n,\"-\", bin(n)[2:].zfill(7),\"-\", \"k\", \"-\", k, \"-\", bin(k)[2:].zfill(7),\"-\", \"POS\", \"-\", t, \"-\", bin(t)[2:].zfill(6))\n",
        "    enc.append(bin(1)[2:]+bin(n)[2:].zfill(7)+bin(k)[2:].zfill(7)+bin(t)[2:].zfill(6))\n",
        "    print(\"Encoded Term (Unique Words):\", (bin(1)[2:]+bin(n)[2:].zfill(7)+bin(k)[2:].zfill(7)+bin(t)[2:].zfill(6)))\n",
        "print('Total Unique Words:', count)\n",
        "print('Unique Vectors')\n",
        "print(enc)\n",
        "#print('And in Binary:', bin(count)[2:].zfill(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SAQ9lvMvsiu",
        "outputId": "2dcf6239-8a66-4818-88cf-af5ac2050dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to play. Do you want to play ??\n",
            "[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('play', 'VB'), ('Do', 'NNP'), ('you', 'PRP'), ('want', 'VB'), ('to', 'TO'), ('play', 'VB')]\n",
            "I - n - 1 - 0000001 - k - 1 - 0000001 - POS - 18 - 010010\n",
            "Encoded Term (Unique Words): 100000010000001010010\n",
            "want - n - 2 - 0000010 - k - 2 - 0000010 - POS - 12 - 001100\n",
            "Encoded Term (Unique Words): 100000100000010001100\n",
            "to - n - 3 - 0000011 - k - 2 - 0000010 - POS - 24 - 011000\n",
            "Encoded Term (Unique Words): 100000110000010011000\n",
            "play - n - 4 - 0000100 - k - 2 - 0000010 - POS - 12 - 001100\n",
            "Encoded Term (Unique Words): 100001000000010001100\n",
            "Do - n - 5 - 0000101 - k - 1 - 0000001 - POS - 30 - 011110\n",
            "Encoded Term (Unique Words): 100001010000001011110\n",
            "you - n - 6 - 0000110 - k - 1 - 0000001 - POS - 18 - 010010\n",
            "Encoded Term (Unique Words): 100001100000001010010\n",
            "Total Unique Words: 6\n",
            "Unique Vectors\n",
            "['100000010000001010010', '100000100000010001100', '100000110000010011000', '100001000000010001100', '100001010000001011110', '100001100000001010010']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ar_unique = ['100000010000001010010', '100000100000010001100', '100000110000010011000', '100001000000010001100', '100001010000001011110', '100001100000001010010']\n",
        "ar_unique_final = []\n",
        "for i in ar_unique:\n",
        "  if i.isdigit():\n",
        "    ar_unique_final.append(int(i))\n",
        "  else:\n",
        "    ar_unique_final.append(i)\n",
        "ar_unique_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gi5TB6vwzke",
        "outputId": "8f65129d-cb38-4e94-cb82-7509cd9bfc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[100000010000001010010,\n",
              " 100000100000010001100,\n",
              " 100000110000010011000,\n",
              " 100001000000010001100,\n",
              " 100001010000001011110,\n",
              " 100001100000001010010]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding for unique vectors"
      ],
      "metadata": {
        "id": "lGYedKCKx_4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [100000010000001010010,\n",
        " 100000100000010001100,\n",
        " 100000110000010011000,\n",
        " 100001000000010001100,\n",
        " 100001010000001011110,\n",
        " 100001100000001010010]\n",
        "b = len(a)\n",
        "c = 69 - b\n",
        "a.extend([111111111111111111111]*c)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQBnBa6Cx8No",
        "outputId": "ae34caf2-c2f0-4820-81fc-581a8b43f5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100000010000001010010, 100000100000010001100, 100000110000010011000, 100001000000010001100, 100001010000001011110, 100001100000001010010, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Root words"
      ],
      "metadata": {
        "id": "D0NbN7seyjDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import TRUE\n",
        "from nltk.corpus.reader import wordlist\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer as las\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "enc=[]\n",
        "ps=PorterStemmer()\n",
        "count = 0\n",
        "n=0\n",
        "en=0\n",
        "t=0\n",
        "file = open(\"Testing.txt\", \"r\")\n",
        "read_dataing = file.read()\n",
        "print(read_dataing)\n",
        "read_data = re.sub(r'[^\\w\\s]', '', read_dataing)\n",
        "text = word_tokenize(read_data)\n",
        "file=open(\"Testing.txt\")\n",
        "my_lines_list=file.readlines()\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(ps.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "print(\"Rooted sentence\")\n",
        "x=stemSentence(my_lines_list[0])\n",
        "read_data_root = re.sub(r'[^\\w\\s]', '', x)\n",
        "textroot = word_tokenize(x)\n",
        "print(x)\n",
        "M1 = word_tokenize(read_data_root)\n",
        "sen = nltk.pos_tag(M1)\n",
        "print(sen)\n",
        "words = Counter(read_data_root.split())\n",
        "for word in words:\n",
        "    count += 1\n",
        "for word in words:\n",
        "    n += 1\n",
        "    en=0\n",
        "    j=words[word]\n",
        "    token_comment = word_tokenize(word)\n",
        "    tagged_comment = pos_tag(token_comment)\n",
        "    for i in range(len(token_comment)):\n",
        "      for (word, tag) in tagged_comment:\n",
        "        if (tag=='CC'):\n",
        "          t=1\n",
        "        if (tag=='CD'):\n",
        "          t=2\n",
        "        if (tag=='DT'):\n",
        "          t=3\n",
        "        if (tag=='EX'):\n",
        "          t=4\n",
        "        if (tag=='FW'):\n",
        "          t=5\n",
        "        if (tag=='IN'):\n",
        "          t=6\n",
        "        if (tag=='JJ'):\n",
        "          t=7\n",
        "        if (tag=='JJR'):\n",
        "          t=8\n",
        "        if (tag=='JJS'):\n",
        "          t=9\n",
        "        if (tag=='LS'):\n",
        "          t=10\n",
        "        if (tag=='MD'):\n",
        "          t=11\n",
        "        if (tag=='NN'):\n",
        "          t=12\n",
        "        if (tag=='NNS'):\n",
        "          t=13\n",
        "        if (tag=='NNP'):\n",
        "          t=14\n",
        "        if (tag=='NNPS'):\n",
        "          t=15\n",
        "        if (tag=='PDT'):\n",
        "          t=16\n",
        "        if (tag=='POS'):\n",
        "          t=17\n",
        "        if (tag=='PRP'):\n",
        "          t=18\n",
        "        if (tag=='PRP$'):\n",
        "          t=19\n",
        "        if (tag=='RB'):\n",
        "          t=20\n",
        "        if (tag=='RBR'):\n",
        "          t=21\n",
        "        if (tag=='RBS'):\n",
        "          t=22\n",
        "        if (tag=='RP'):\n",
        "          t=23\n",
        "        if (tag=='TO'):\n",
        "          t=24\n",
        "        if (tag=='UH'):\n",
        "          t=25\n",
        "        if (tag=='VBP'):\n",
        "          t=26\n",
        "        if (tag=='VBD'):\n",
        "          t=27\n",
        "        if (tag=='VBG'):\n",
        "          t=28\n",
        "        if (tag=='VBN'):\n",
        "          t=29\n",
        "        if (tag=='VB'):\n",
        "          t=30\n",
        "        if (tag=='VBZ'):\n",
        "          t=31\n",
        "        if (tag=='WDT'):\n",
        "          t=32\n",
        "        if (tag=='WP'):\n",
        "          t=33\n",
        "        if (tag=='WP$'):\n",
        "          t=34\n",
        "        if (tag=='WRB'):\n",
        "          t=35\n",
        "    en=n+j+t\n",
        "    print(word, \"-\", \"n\", \"-\", n,\"-\", bin(n)[2:].zfill(7),\"-\", \"k\", \"-\", k, \"-\", bin(k)[2:].zfill(7), \"-\", \"POS\", \"-\", t, \"-\", bin(t)[2:].zfill(6))\n",
        "    enc.append(bin(1)[2:]+bin(n)[2:].zfill(7)+bin(k)[2:].zfill(7)+bin(t)[2:].zfill(6))\n",
        "    print(\"Encoded Term (Rooted Words):\", (bin(1)[2:]+bin(n)[2:].zfill(7)+bin(k)[2:].zfill(7)+bin(t)[2:].zfill(6)))\n",
        "print('Total Root Words:', count)\n",
        "print('Root Vectors')\n",
        "print(enc)\n",
        "#print('And in Binary:', bin(count)[2:].zfill(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB9iGfz_ycQd",
        "outputId": "b46ad671-156f-4452-818a-6f0768ef9770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to play. Do you want to play ??\n",
            "Rooted sentence\n",
            "I want to play . Do you want to play ? ? \n",
            "[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('play', 'VB'), ('Do', 'NNP'), ('you', 'PRP'), ('want', 'VB'), ('to', 'TO'), ('play', 'VB')]\n",
            "I - n - 1 - 0000001 - k - 1 - 0000001 - POS - 18 - 010010\n",
            "Encoded Term (Rooted Words): 100000010000001010010\n",
            "want - n - 2 - 0000010 - k - 1 - 0000001 - POS - 12 - 001100\n",
            "Encoded Term (Rooted Words): 100000100000001001100\n",
            "to - n - 3 - 0000011 - k - 1 - 0000001 - POS - 24 - 011000\n",
            "Encoded Term (Rooted Words): 100000110000001011000\n",
            "play - n - 4 - 0000100 - k - 1 - 0000001 - POS - 12 - 001100\n",
            "Encoded Term (Rooted Words): 100001000000001001100\n",
            "Do - n - 5 - 0000101 - k - 1 - 0000001 - POS - 30 - 011110\n",
            "Encoded Term (Rooted Words): 100001010000001011110\n",
            "you - n - 6 - 0000110 - k - 1 - 0000001 - POS - 18 - 010010\n",
            "Encoded Term (Rooted Words): 100001100000001010010\n",
            "Total Root Words: 6\n",
            "Root Vectors\n",
            "['100000010000001010010', '100000100000001001100', '100000110000001011000', '100001000000001001100', '100001010000001011110', '100001100000001010010']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ar_root = ['100000010000001010010', '100000100000001001100', '100000110000001011000', '100001000000001001100', '100001010000001011110', '100001100000001010010']\n",
        "ar_root_final = []\n",
        "for i in ar_root:\n",
        "  if i.isdigit():\n",
        "    ar_root_final.append(int(i))\n",
        "  else:\n",
        "    ar_root_final.append(i)\n",
        "ar_root_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGB0pezwzQu1",
        "outputId": "53b37e76-a97c-4c51-d688-785d3ae20fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[100000010000001010010,\n",
              " 100000100000001001100,\n",
              " 100000110000001011000,\n",
              " 100001000000001001100,\n",
              " 100001010000001011110,\n",
              " 100001100000001010010]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding for root vectors"
      ],
      "metadata": {
        "id": "PrXWfK2A0BU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [100000010000001010010,\n",
        " 100000100000001001100,\n",
        " 100000110000001011000,\n",
        " 100001000000001001100,\n",
        " 100001010000001011110,\n",
        " 100001100000001010010]\n",
        "b = len(a)\n",
        "c = 69 - b\n",
        "a.extend([111111111111111111111]*c)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw6LlNy0zrfl",
        "outputId": "46d1394b-9add-469b-d11c-1526dfca4936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100000010000001010010, 100000100000001001100, 100000110000001011000, 100001000000001001100, 100001010000001011110, 100001100000001010010, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check similarity"
      ],
      "metadata": {
        "id": "uuKEgpo30Med"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "import tensorflow as tf\n",
        "unique_vector = [100000010000001010010, 100000100000010001100, 100000110000010011000, 100001000000010001100, 100001010000001011110, 100001100000001010010, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111]\n",
        "root_vector = [100000010000001010010, 100000100000001001100, 100000110000001011000, 100001000000001001100, 100001010000001011110, 100001100000001010010, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111, 111111111111111111111]\n",
        "X = np.array(unique_vector).astype(float)\n",
        "Y = np.array(root_vector).astype(float)\n",
        "print(\"Cosine similarity =\", abs(1-distance.cosine(X, Y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmub4gIdz98A",
        "outputId": "ceb7b429-7170-42a2-bcca-dc1cb5972038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity = 0.9999999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1o_eLea05Su"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}